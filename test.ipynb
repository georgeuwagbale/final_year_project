{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T19:04:35.077581Z",
     "start_time": "2024-07-14T19:04:29.767422Z"
    },
    "collapsed": true,
    "id": "initial_id",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "hmhPDYGbtWbD",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T19:04:35.093308Z",
     "start_time": "2024-07-14T19:04:35.081766Z"
    },
    "id": "hmhPDYGbtWbD"
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.stride = stride\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(input_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(input_channels, int(\n",
    "            output_channels/4), 1, 1, bias=False)\n",
    "\n",
    "        self.bn2 = nn.BatchNorm2d(int(output_channels/4))\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            int(output_channels/4), int(output_channels/4), 3, stride, padding=1, bias=False)\n",
    "\n",
    "        self.bn3 = nn.BatchNorm2d(int(output_channels/4))\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv3 = nn.Conv2d(int(output_channels/4),\n",
    "                               output_channels, 1, 1, bias=False)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(\n",
    "            input_channels, output_channels, 1, stride, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.bn1(x)\n",
    "        out1 = self.relu(out)\n",
    "        out = self.conv1(out1)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)\n",
    "\n",
    "        if (self.input_channels != self.output_channels) or (self.stride != 1):\n",
    "            residual = self.conv4(out1)\n",
    "        out += residual\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "NHHiMbwEt0z5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T19:04:35.119683Z",
     "start_time": "2024-07-14T19:04:35.095840Z"
    },
    "id": "NHHiMbwEt0z5"
   },
   "outputs": [],
   "source": [
    "class AttentionModule_stage0(nn.Module):\n",
    "    # input size is 112*112\n",
    "    def __init__(self, in_channels, out_channels, size1=(112, 112), size2=(56, 56), size3=(28, 28), size4=(14, 14)):\n",
    "        super(AttentionModule_stage0, self).__init__()\n",
    "        self.first_residual_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "        self.trunk_branches = nn.Sequential(\n",
    "            ResidualBlock(in_channels, out_channels),\n",
    "            ResidualBlock(in_channels, out_channels)\n",
    "         )\n",
    "\n",
    "        self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        # 56*56\n",
    "        self.softmax1_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "        self.skip1_connection_residual_block = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "        self.mpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        # 28*28\n",
    "        self.softmax2_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "        self.skip2_connection_residual_block = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "        self.mpool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        # 14*14\n",
    "        self.softmax3_blocks = ResidualBlock(in_channels, out_channels)\n",
    "        self.skip3_connection_residual_block = ResidualBlock(in_channels, out_channels)\n",
    "        self.mpool4 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        # 7*7\n",
    "        self.softmax4_blocks = nn.Sequential(\n",
    "            ResidualBlock(in_channels, out_channels),\n",
    "            ResidualBlock(in_channels, out_channels)\n",
    "        )\n",
    "        self.interpolation4 = nn.UpsamplingBilinear2d(size=size4)\n",
    "        self.softmax5_blocks = ResidualBlock(in_channels, out_channels)\n",
    "        self.interpolation3 = nn.UpsamplingBilinear2d(size=size3)\n",
    "        self.softmax6_blocks = ResidualBlock(in_channels, out_channels)\n",
    "        self.interpolation2 = nn.UpsamplingBilinear2d(size=size2)\n",
    "        self.softmax7_blocks = ResidualBlock(in_channels, out_channels)\n",
    "        self.interpolation1 = nn.UpsamplingBilinear2d(size=size1)\n",
    "\n",
    "        self.softmax8_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias = False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels , kernel_size=1, stride=1, bias = False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.last_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 112*112\n",
    "        x = self.first_residual_blocks(x)\n",
    "        out_trunk = self.trunk_branches(x)\n",
    "        out_mpool1 = self.mpool1(x)\n",
    "        # 56*56\n",
    "        out_softmax1 = self.softmax1_blocks(out_mpool1)\n",
    "        out_skip1_connection = self.skip1_connection_residual_block(out_softmax1)\n",
    "        out_mpool2 = self.mpool2(out_softmax1)\n",
    "        # 28*28\n",
    "        out_softmax2 = self.softmax2_blocks(out_mpool2)\n",
    "        out_skip2_connection = self.skip2_connection_residual_block(out_softmax2)\n",
    "        out_mpool3 = self.mpool3(out_softmax2)\n",
    "        # 14*14\n",
    "        out_softmax3 = self.softmax3_blocks(out_mpool3)\n",
    "        out_skip3_connection = self.skip3_connection_residual_block(out_softmax3)\n",
    "        out_mpool4 = self.mpool4(out_softmax3)\n",
    "        # 7*7\n",
    "        out_softmax4 = self.softmax4_blocks(out_mpool4)\n",
    "        self.interpolation4 = nn.UpsamplingBilinear2d(size=out_softmax3.size()[2:])\n",
    "        out_interp4 = self.interpolation4(out_softmax4) + out_softmax3\n",
    "        out = out_interp4 + out_skip3_connection\n",
    "        out_softmax5 = self.softmax5_blocks(out)\n",
    "        self.interpolation3 = nn.UpsamplingBilinear2d(size=out_softmax2.size()[2:])\n",
    "        out_interp3 = self.interpolation3(out_softmax5) + out_softmax2\n",
    "        # print(out_skip2_connection.data)\n",
    "        # print(out_interp3.data)\n",
    "        out = out_interp3 + out_skip2_connection\n",
    "        out_softmax6 = self.softmax6_blocks(out)\n",
    "        self.interpolation2 = nn.UpsamplingBilinear2d(size=out_softmax1.size()[2:])\n",
    "        out_interp2 = self.interpolation2(out_softmax6) + out_softmax1\n",
    "        out = out_interp2 + out_skip1_connection\n",
    "        out_softmax7 = self.softmax7_blocks(out)\n",
    "        self.interpolation1 = nn.UpsamplingBilinear2d(size=out_trunk.size()[2:])\n",
    "        out_interp1 = self.interpolation1(out_softmax7) + out_trunk\n",
    "        out_softmax8 = self.softmax8_blocks(out_interp1)\n",
    "        out = (1 + out_softmax8) * out_trunk\n",
    "        out_last = self.last_blocks(out)\n",
    "\n",
    "        return out_last\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "zGTFzHusv0i4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T19:04:35.220188Z",
     "start_time": "2024-07-14T19:04:35.123300Z"
    },
    "id": "zGTFzHusv0i4"
   },
   "outputs": [],
   "source": [
    "class AttentionModule_stage1(nn.Module):\n",
    "    # input size is 56*56\n",
    "    def __init__(self, in_channels, out_channels, size1=(56, 56), size2=(28, 28), size3=(14, 14)):\n",
    "        super(AttentionModule_stage1, self).__init__()\n",
    "        self.first_residual_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "        self.trunk_branches = nn.Sequential(\n",
    "            ResidualBlock(in_channels, out_channels),\n",
    "            ResidualBlock(in_channels, out_channels)\n",
    "         )\n",
    "\n",
    "        self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.softmax1_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "        self.skip1_connection_residual_block = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "        self.mpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.softmax2_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "        self.skip2_connection_residual_block = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "        self.mpool3 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.softmax3_blocks = nn.Sequential(\n",
    "            ResidualBlock(in_channels, out_channels),\n",
    "            ResidualBlock(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "        self.interpolation3 = nn.UpsamplingBilinear2d(size=size3)\n",
    "\n",
    "        self.softmax4_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "        self.interpolation2 = nn.UpsamplingBilinear2d(size=size2)\n",
    "\n",
    "        self.softmax5_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "        self.interpolation1 = nn.UpsamplingBilinear2d(size=size1)\n",
    "\n",
    "        self.softmax6_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels , kernel_size = 1, stride = 1, bias = False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels , kernel_size = 1, stride = 1, bias = False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.last_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_residual_blocks(x)\n",
    "        out_trunk = self.trunk_branches(x)\n",
    "        out_mpool1 = self.mpool1(x)\n",
    "        out_softmax1 = self.softmax1_blocks(out_mpool1)\n",
    "        out_skip1_connection = self.skip1_connection_residual_block(out_softmax1)\n",
    "        out_mpool2 = self.mpool2(out_softmax1)\n",
    "        out_softmax2 = self.softmax2_blocks(out_mpool2)\n",
    "        out_skip2_connection = self.skip2_connection_residual_block(out_softmax2)\n",
    "        out_mpool3 = self.mpool3(out_softmax2)\n",
    "        out_softmax3 = self.softmax3_blocks(out_mpool3)\n",
    "        #\n",
    "        self.interpolation3 = nn.UpsamplingBilinear2d(size=out_softmax2.size()[2:])\n",
    "        out_interp3 = self.interpolation3(out_softmax3) + out_softmax2\n",
    "        # print(out_skip2_connection.data)\n",
    "        # print(out_interp3.data)\n",
    "        out = out_interp3 + out_skip2_connection\n",
    "        out_softmax4 = self.softmax4_blocks(out)\n",
    "\n",
    "        self.interpolation2 = nn.UpsamplingBilinear2d(size=out_softmax1.size()[2:])\n",
    "        out_interp2 = self.interpolation2(out_softmax4) + out_softmax1\n",
    "        out = out_interp2 + out_skip1_connection\n",
    "        out_softmax5 = self.softmax5_blocks(out)\n",
    "\n",
    "        self.interpolation1 = nn.UpsamplingBilinear2d(size=out_trunk.size()[2:])\n",
    "        out_interp1 = self.interpolation1(out_softmax5) + out_trunk\n",
    "        out_softmax6 = self.softmax6_blocks(out_interp1)\n",
    "        out = (1 + out_softmax6) * out_trunk\n",
    "        out_last = self.last_blocks(out)\n",
    "\n",
    "        return out_last\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "NZDJ_uvQ0Cku",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T19:04:35.236994Z",
     "start_time": "2024-07-14T19:04:35.224985Z"
    },
    "id": "NZDJ_uvQ0Cku"
   },
   "outputs": [],
   "source": [
    "class AttentionModule_stage2(nn.Module):\n",
    "    # input image size is 28*28\n",
    "    def __init__(self, in_channels, out_channels, size1=(28, 28), size2=(14, 14)):\n",
    "        super(AttentionModule_stage2, self).__init__()\n",
    "        self.first_residual_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "        self.trunk_branches = nn.Sequential(\n",
    "            ResidualBlock(in_channels, out_channels),\n",
    "            ResidualBlock(in_channels, out_channels)\n",
    "         )\n",
    "\n",
    "        self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.softmax1_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "        self.skip1_connection_residual_block = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "        self.mpool2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.softmax2_blocks = nn.Sequential(\n",
    "            ResidualBlock(in_channels, out_channels),\n",
    "            ResidualBlock(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "        self.interpolation2 = nn.UpsamplingBilinear2d(size=size2)\n",
    "\n",
    "        self.softmax3_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "        self.interpolation1 = nn.UpsamplingBilinear2d(size=size1)\n",
    "\n",
    "        self.softmax4_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.last_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_residual_blocks(x)\n",
    "        out_trunk = self.trunk_branches(x)\n",
    "        out_mpool1 = self.mpool1(x)\n",
    "        out_softmax1 = self.softmax1_blocks(out_mpool1)\n",
    "        out_skip1_connection = self.skip1_connection_residual_block(out_softmax1)\n",
    "        out_mpool2 = self.mpool2(out_softmax1)\n",
    "        out_softmax2 = self.softmax2_blocks(out_mpool2)\n",
    "\n",
    "        self.interpolation2 = nn.UpsamplingBilinear2d(size=out_softmax1.size()[2:])\n",
    "        out_interp2 = self.interpolation2(out_softmax2) + out_softmax1\n",
    "        # print(out_skip2_connection.data)\n",
    "        # print(out_interp3.data)\n",
    "        out = out_interp2 + out_skip1_connection\n",
    "        out_softmax3 = self.softmax3_blocks(out)\n",
    "\n",
    "        self.interpolation1 = nn.UpsamplingBilinear2d(size=out_trunk.size()[2:])\n",
    "        out_interp1 = self.interpolation1(out_softmax3) + out_trunk\n",
    "        out_softmax4 = self.softmax4_blocks(out_interp1)\n",
    "        out = (1 + out_softmax4) * out_trunk\n",
    "        out_last = self.last_blocks(out)\n",
    "\n",
    "        return out_last\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "haNzYTer0ESF",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T19:04:35.275992Z",
     "start_time": "2024-07-14T19:04:35.249645Z"
    },
    "id": "haNzYTer0ESF"
   },
   "outputs": [],
   "source": [
    "class AttentionModule_stage3(nn.Module):\n",
    "    # input image size is 14*14\n",
    "    def __init__(self, in_channels, out_channels, size1=(14, 14)):\n",
    "        super(AttentionModule_stage3, self).__init__()\n",
    "        self.first_residual_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "        self.trunk_branches = nn.Sequential(\n",
    "            ResidualBlock(in_channels, out_channels),\n",
    "            ResidualBlock(in_channels, out_channels)\n",
    "         )\n",
    "\n",
    "        self.mpool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.softmax1_blocks = nn.Sequential(\n",
    "            ResidualBlock(in_channels, out_channels),\n",
    "            ResidualBlock(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "        self.interpolation1 = nn.UpsamplingBilinear2d(size=size1)\n",
    "\n",
    "        self.softmax2_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.last_blocks = ResidualBlock(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_residual_blocks(x)\n",
    "        out_trunk = self.trunk_branches(x)\n",
    "        out_mpool1 = self.mpool1(x)\n",
    "        out_softmax1 = self.softmax1_blocks(out_mpool1)\n",
    "\n",
    "        self.interpolation1 = nn.UpsamplingBilinear2d(size=out_trunk.size()[2:])\n",
    "        out_interp1 = self.interpolation1(out_softmax1) + out_trunk\n",
    "        out_softmax2 = self.softmax2_blocks(out_interp1)\n",
    "        out = (1 + out_softmax2) * out_trunk\n",
    "        out_last = self.last_blocks(out)\n",
    "\n",
    "        return out_last\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "xH1OaJ700KET",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T19:04:35.293075Z",
     "start_time": "2024-07-14T19:04:35.278334Z"
    },
    "id": "xH1OaJ700KET"
   },
   "outputs": [],
   "source": [
    "class ResidualAttentionModel(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResidualAttentionModel, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # Remove fixed-size max-pooling layer\n",
    "        self.mpool1 = nn.Sequential(\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveMaxPool2d((None, None))  # Adaptive pooling\n",
    "        )\n",
    "        self.residual_block0 = ResidualBlock(64, 128)\n",
    "        self.attention_module0 = AttentionModule_stage0(128, 128)\n",
    "        self.residual_block1 = ResidualBlock(128, 256, 2)\n",
    "        self.attention_module1 = AttentionModule_stage1(256, 256)\n",
    "        self.residual_block2 = ResidualBlock(256, 512, 2)\n",
    "        self.attention_module2 = AttentionModule_stage2(512, 512)\n",
    "        self.attention_module2_2 = AttentionModule_stage2(512, 512)\n",
    "        self.residual_block3 = ResidualBlock(512, 1024, 2)\n",
    "        self.attention_module3 = AttentionModule_stage3(1024, 1024)\n",
    "        self.attention_module3_2 = AttentionModule_stage3(1024, 1024)\n",
    "        self.attention_module3_3 = AttentionModule_stage3(1024, 1024)\n",
    "        self.residual_block4 = ResidualBlock(1024, 2048, 2)\n",
    "        self.residual_block5 = ResidualBlock(2048, 2048)\n",
    "        self.residual_block6 = ResidualBlock(2048, 2048)\n",
    "        # Remove fixed-size average pooling layer\n",
    "        self.mpool2 = nn.Sequential(\n",
    "            nn.BatchNorm2d(2048),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1)  # Adaptive pooling\n",
    "        )\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.mpool1(out)\n",
    "        out = self.residual_block0(out)\n",
    "        out = self.attention_module0(out)\n",
    "        out = self.residual_block1(out)\n",
    "        out = self.attention_module1(out)\n",
    "        out = self.residual_block2(out)\n",
    "        out = self.attention_module2(out)\n",
    "        out = self.attention_module2_2(out)\n",
    "        out = self.residual_block3(out)\n",
    "        out = self.attention_module3(out)\n",
    "        out = self.attention_module3_2(out)\n",
    "        out = self.attention_module3_3(out)\n",
    "        out = self.residual_block4(out)\n",
    "        out = self.residual_block5(out)\n",
    "        out = self.residual_block6(out)\n",
    "        out = self.mpool2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b9789de8de1241a4",
   "metadata": {
    "id": "b9789de8de1241a4"
   },
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# shape = (5, 3, 150, 2500)\n",
    "# data = torch.randn(shape).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "62d9429f4524aa05",
   "metadata": {
    "id": "62d9429f4524aa05"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# model = ResidualAttentionModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e45738c108a9fef6",
   "metadata": {
    "id": "e45738c108a9fef6"
   },
   "outputs": [],
   "source": [
    "# # output = model(data)\n",
    "# output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fccdba21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T19:04:37.416244Z",
     "start_time": "2024-07-14T19:04:35.347410Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class SteelDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
    "        self.images = self._load_images()\n",
    "\n",
    "    def _load_images(self):\n",
    "        images = []\n",
    "        for cls in self.classes:\n",
    "            class_dir = os.path.join(self.root_dir, cls)\n",
    "            for filename in os.listdir(class_dir):\n",
    "                if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "                    images.append((os.path.join(class_dir, filename), cls))\n",
    "        return images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.images[idx]\n",
    "        image = Image.open(img_path).convert('L')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.class_to_idx[label]\n",
    "        return image, label\n",
    "\n",
    "# Define transformations to apply to the images\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),  # Resize images to a fixed size\n",
    "#     transforms.ToTensor(),           # Convert images to PyTorch tensors\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize images\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ca07928f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_dir = \"C:/Users/DELL/Desktop/FYP/codes/Residual-Attention-Network/IMAGES/DEFECTS\" #\"./IMAGES\"\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "# dataset = SteelDataset(root_dir, transform=transform)\n",
    "# dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a7f08ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d8e91199",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-14T19:04:37.418781Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[110], line 141\u001b[0m\n\u001b[0;32m    139\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m    140\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m--> 141\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    142\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m22\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\DELL\\miniconda3\\envs\\ResidualAttentionNetwork-pytorch\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\DELL\\miniconda3\\envs\\ResidualAttentionNetwork-pytorch\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     tensors,\n\u001b[0;32m    268\u001b[0m     grad_tensors_,\n\u001b[0;32m    269\u001b[0m     retain_graph,\n\u001b[0;32m    270\u001b[0m     create_graph,\n\u001b[0;32m    271\u001b[0m     inputs,\n\u001b[0;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models, datasets\n",
    "import os\n",
    "# import cv2\n",
    "import time\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# from model.residual_attention_network_pre import ResidualAttentionModel\n",
    "\n",
    "\n",
    "model_file = 'model_92_sgd.pkl'\n",
    "# print(\"Hello\")\n",
    "\n",
    "# for test\n",
    "def test(model, test_loader, btrain=False, model_file='model_92.pkl'):\n",
    "    # Test\n",
    "    if not btrain:\n",
    "        model.load_state_dict(torch.load(model_file))\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    #\n",
    "    class_correct = list(0. for i in range(7))\n",
    "    class_total = list(0. for i in range(7))\n",
    "\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.data).sum()\n",
    "        \n",
    "        all_preds.extend(predicted.numpy())\n",
    "        all_labels.extend(labels.data.numpy())\n",
    "\n",
    "        #\n",
    "        c = (predicted == labels.data).squeeze()\n",
    "        for i in range(labels.size(0)):\n",
    "            label = labels.data[i]\n",
    "            class_correct[label] += c[i]\n",
    "            class_total[label] += 1\n",
    "\n",
    "    print('Accuracy of the model on the test images: %d %%' % (100 * float(correct) / total))\n",
    "    print('Accuracy of the model on the test images:', float(correct)/total)\n",
    "    \n",
    "    # Calculate Precision, Recall and F1 Score\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    \n",
    "    print('Precision: ', precision)\n",
    "    print('Recall: ', recall)\n",
    "    print('F1 Score: ', f1)\n",
    "    \n",
    "    for i in range(7):\n",
    "        if class_total[i] != 0:\n",
    "            print('Accuracy of %5s : %2d %%' % (\n",
    "                classes[i], 100 * class_correct[i] / class_total[i]))\n",
    "        else:\n",
    "            print('No instances of %5s in the test set.\\n' % (classes[i]))\n",
    "    return correct / total\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "# Image Preprocessing\n",
    "# data_transforms2 = transforms.Compose([\n",
    "#             transforms.Resize(size=(150,2300)),\n",
    "#             transforms.Grayscale(1),\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize([0.5, 0.7, 0.5], [0.222, 0.226, 0.333]) ])   \n",
    "\n",
    "\n",
    "# The purpose of normalization is to scale your image pixel values to have a mean of 0 and a standard deviation of 1,\n",
    "# which helps in speeding up the convergence during the training of neural networks.\n",
    "data_transforms2 = transforms.Compose([\n",
    "            transforms.Resize(size=(150,2300)),\n",
    "            transforms.Grayscale(1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.222])])   \n",
    "\n",
    "# test_transform = transforms.Compose([\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "\n",
    "# Define paths to your image directories\n",
    "root_dir = \"C:/Users/DELL/Desktop/FYP/codes/Residual-Attention-Network/IMAGES/DEFECTS\" #\"./IMAGES\"\n",
    "dataset = SteelDataset(root_dir, transform=data_transforms2)\n",
    "\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                                        batch_size=2, # 2\n",
    "                                        shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                                        batch_size=2,\n",
    "                                        shuffle=False)\n",
    "\n",
    "\n",
    "classes = ('broken', 'cracks','dents', 'depressions','hairs', \"incrustations\", \"irregular overlaps\")\n",
    "n_classes = len(classes)\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "model = ResidualAttentionModel(n_classes).to(device)\n",
    "# print(model)\n",
    "\n",
    "lr = 0.1  # 0.1\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, nesterov=True, weight_decay=0.0001)\n",
    "is_train = True\n",
    "is_pretrain = False\n",
    "acc_best = 0\n",
    "total_epoch = 50\n",
    "if is_train is True:\n",
    "    if is_pretrain:\n",
    "        model.load_state_dict((torch.load(model_file)))\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(total_epoch):\n",
    "        \n",
    "        model.train()\n",
    "        tims = time.time()\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (i+1) % 22 == 0:\n",
    "                print(\"Epoch [%d/%d], Iter [%d/%d] Loss: %.4f\" %(epoch+1, total_epoch, i+1, len(train_loader), loss.item()))\n",
    "                \n",
    "        print('This epoch took:',time.time()-tims)\n",
    "        print('\\nEvaluate test set:')\n",
    "        acc = test(model, test_loader, btrain=True)\n",
    "        if acc > acc_best:\n",
    "            acc_best = acc\n",
    "            print('current best acc,', acc_best)\n",
    "            torch.save(model.state_dict(), model_file)\n",
    "\n",
    "        # Decaying Learning Rate\n",
    "        if (epoch+1) / float(total_epoch) == 0.3 or (epoch+1) / float(total_epoch) == 0.6 or (epoch+1) / float(total_epoch) == 0.9:\n",
    "            lr /= 10\n",
    "            print('reset learning rate to:', lr)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "                print(param_group['lr'])\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "            optim.SGD(model.parameters(), lr=lr, momentum=0.9, nesterov=True, weight_decay=0.0001)\n",
    "            \n",
    "    # Save the Model\n",
    "    torch.save(model.state_dict(), 'last_model_92_sgd.pkl')\n",
    "\n",
    "else:\n",
    "    test(model, test_loader, btrain=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4613d5032d312234",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T20:03:02.800479Z",
     "start_time": "2024-07-14T20:03:02.547847Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac01677",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
